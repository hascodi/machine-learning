<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Hoofdstuk 6 De percetron | Machine Learning</title>
  <meta name="description" content="Artificial intelligence course at the AP University College." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Hoofdstuk 6 De percetron | Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Artificial intelligence course at the AP University College." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Hoofdstuk 6 De percetron | Machine Learning" />
  
  <meta name="twitter:description" content="Artificial intelligence course at the AP University College." />
  

<meta name="author" content="34142/1916/2021/1/38 David D’Haese" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="img/favicon.ico" type="image/x-icon" />
<link rel="prev" href="manipuleren-van-data.html"/>
<link rel="next" href="inleiding-tot-artificiële-neurale-netwerken.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css\course.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="inleiding-tot-de-cursus.html"><a href="inleiding-tot-de-cursus.html"><i class="fa fa-check"></i><b>1</b> Inleiding tot de cursus</a><ul>
<li class="chapter" data-level="1.1" data-path="inleiding-tot-de-cursus.html"><a href="inleiding-tot-de-cursus.html#in-een-notedop"><i class="fa fa-check"></i><b>1.1</b> In een notedop</a></li>
<li class="chapter" data-level="1.2" data-path="inleiding-tot-de-cursus.html"><a href="inleiding-tot-de-cursus.html#leerdoelen"><i class="fa fa-check"></i><b>1.2</b> Leerdoelen</a></li>
<li class="chapter" data-level="1.3" data-path="inleiding-tot-de-cursus.html"><a href="inleiding-tot-de-cursus.html#cursus-vorm"><i class="fa fa-check"></i><b>1.3</b> Cursus vorm</a></li>
<li class="chapter" data-level="1.4" data-path="inleiding-tot-de-cursus.html"><a href="inleiding-tot-de-cursus.html#bekijken-van-deze-cursus"><i class="fa fa-check"></i><b>1.4</b> Bekijken van deze Cursus</a></li>
<li class="chapter" data-level="1.5" data-path="inleiding-tot-de-cursus.html"><a href="inleiding-tot-de-cursus.html#code-uit-de-cursus-uitvoeren"><i class="fa fa-check"></i><b>1.5</b> Code uit de cursus uitvoeren</a></li>
<li class="chapter" data-level="1.6" data-path="inleiding-tot-de-cursus.html"><a href="inleiding-tot-de-cursus.html#oefeningen-maken"><i class="fa fa-check"></i><b>1.6</b> Oefeningen maken</a></li>
<li class="chapter" data-level="1.7" data-path="inleiding-tot-de-cursus.html"><a href="inleiding-tot-de-cursus.html#licentie-voor-deze-cursus"><i class="fa fa-check"></i><b>1.7</b> Licentie voor deze cursus</a></li>
<li class="chapter" data-level="1.8" data-path="inleiding-tot-de-cursus.html"><a href="inleiding-tot-de-cursus.html#verwijzen-naar-deze-cursus"><i class="fa fa-check"></i><b>1.8</b> Verwijzen naar deze cursus</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="leren-uit-data.html"><a href="leren-uit-data.html"><i class="fa fa-check"></i><b>2</b> Leren uit data</a><ul>
<li class="chapter" data-level="2.1" data-path="leren-uit-data.html"><a href="leren-uit-data.html#het-leerproces"><i class="fa fa-check"></i><b>2.1</b> Het leerproces</a></li>
<li class="chapter" data-level="2.2" data-path="leren-uit-data.html"><a href="leren-uit-data.html#de-evolutie-van-het-machinaal-leren"><i class="fa fa-check"></i><b>2.2</b> De evolutie van het machinaal leren</a></li>
<li class="chapter" data-level="2.3" data-path="leren-uit-data.html"><a href="leren-uit-data.html#intelligentie"><i class="fa fa-check"></i><b>2.3</b> Intelligentie</a></li>
<li class="chapter" data-level="2.4" data-path="leren-uit-data.html"><a href="leren-uit-data.html#het-model"><i class="fa fa-check"></i><b>2.4</b> Het model</a></li>
<li class="chapter" data-level="2.5" data-path="leren-uit-data.html"><a href="leren-uit-data.html#doelfunctie"><i class="fa fa-check"></i><b>2.5</b> Doelfunctie</a></li>
<li class="chapter" data-level="2.6" data-path="leren-uit-data.html"><a href="leren-uit-data.html#mnist-dataset"><i class="fa fa-check"></i><b>2.6</b> MNIST dataset</a></li>
<li class="chapter" data-level="2.7" data-path="leren-uit-data.html"><a href="leren-uit-data.html#het-resultaat-van-mnist-analyse"><i class="fa fa-check"></i><b>2.7</b> Het resultaat van MNIST analyse</a></li>
<li class="chapter" data-level="2.8" data-path="leren-uit-data.html"><a href="leren-uit-data.html#het-mnist-model"><i class="fa fa-check"></i><b>2.8</b> Het MNIST model</a></li>
<li class="chapter" data-level="2.9" data-path="leren-uit-data.html"><a href="leren-uit-data.html#het-leerproces-voor-begeleid-ml"><i class="fa fa-check"></i><b>2.9</b> Het leerproces voor begeleid ML</a></li>
<li class="chapter" data-level="2.10" data-path="leren-uit-data.html"><a href="leren-uit-data.html#de-onderdelen-van-een-model"><i class="fa fa-check"></i><b>2.10</b> De onderdelen van een model</a></li>
<li class="chapter" data-level="2.11" data-path="leren-uit-data.html"><a href="leren-uit-data.html#hyperparameters"><i class="fa fa-check"></i><b>2.11</b> Hyperparameters</a></li>
<li class="chapter" data-level="2.12" data-path="leren-uit-data.html"><a href="leren-uit-data.html#het-leeralgoritme"><i class="fa fa-check"></i><b>2.12</b> Het leeralgoritme</a></li>
<li class="chapter" data-level="2.13" data-path="leren-uit-data.html"><a href="leren-uit-data.html#model-complexiteit"><i class="fa fa-check"></i><b>2.13</b> Model complexiteit</a></li>
<li class="chapter" data-level="2.14" data-path="leren-uit-data.html"><a href="leren-uit-data.html#comprimeren-door-middel-van-een-ml-model"><i class="fa fa-check"></i><b>2.14</b> Comprimeren door middel van een ML model</a></li>
<li class="chapter" data-level="2.15" data-path="leren-uit-data.html"><a href="leren-uit-data.html#leren-versus-ontwerp"><i class="fa fa-check"></i><b>2.15</b> Leren versus ontwerp</a></li>
<li class="chapter" data-level="2.16" data-path="leren-uit-data.html"><a href="leren-uit-data.html#leren-versus-onthouden-en-inferentie"><i class="fa fa-check"></i><b>2.16</b> Leren versus onthouden en inferentie</a></li>
<li class="chapter" data-level="2.17" data-path="leren-uit-data.html"><a href="leren-uit-data.html#onbegeleid-ml"><i class="fa fa-check"></i><b>2.17</b> Onbegeleid ML</a></li>
<li class="chapter" data-level="2.18" data-path="leren-uit-data.html"><a href="leren-uit-data.html#conditionering"><i class="fa fa-check"></i><b>2.18</b> Conditionering</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Data</a><ul>
<li class="chapter" data-level="3.1" data-path="data.html"><a href="data.html#data-voor-ml"><i class="fa fa-check"></i><b>3.1</b> Data voor ML</a></li>
<li class="chapter" data-level="3.2" data-path="data.html"><a href="data.html#wat-is-data"><i class="fa fa-check"></i><b>3.2</b> Wat is data</a></li>
<li class="chapter" data-level="3.3" data-path="data.html"><a href="data.html#soorten-data"><i class="fa fa-check"></i><b>3.3</b> Soorten data</a></li>
<li class="chapter" data-level="3.4" data-path="data.html"><a href="data.html#externe-databronnen"><i class="fa fa-check"></i><b>3.4</b> Externe databronnen</a></li>
<li class="chapter" data-level="3.5" data-path="data.html"><a href="data.html#data-genereren"><i class="fa fa-check"></i><b>3.5</b> Data Genereren</a></li>
<li class="chapter" data-level="3.6" data-path="data.html"><a href="data.html#de-analyse-dataset"><i class="fa fa-check"></i><b>3.6</b> De analyse dataset</a></li>
<li class="chapter" data-level="3.7" data-path="data.html"><a href="data.html#soorten-variabelen"><i class="fa fa-check"></i><b>3.7</b> Soorten variabelen</a></li>
<li class="chapter" data-level="3.8" data-path="data.html"><a href="data.html#eng-nominal-scale-data"><i class="fa fa-check"></i><b>3.8</b> (Eng) Nominal-Scale Data</a></li>
<li class="chapter" data-level="3.9" data-path="data.html"><a href="data.html#eng-dummy-variables"><i class="fa fa-check"></i><b>3.9</b> (Eng) Dummy Variables</a></li>
<li class="chapter" data-level="3.10" data-path="data.html"><a href="data.html#eng-ordinal-scale-data"><i class="fa fa-check"></i><b>3.10</b> (Eng) Ordinal-Scale Data</a></li>
<li class="chapter" data-level="3.11" data-path="data.html"><a href="data.html#eng-circular-scale"><i class="fa fa-check"></i><b>3.11</b> (Eng) Circular-Scale</a></li>
<li class="chapter" data-level="3.12" data-path="data.html"><a href="data.html#eng-censoring"><i class="fa fa-check"></i><b>3.12</b> (Eng) Censoring</a></li>
<li class="chapter" data-level="3.13" data-path="data.html"><a href="data.html#tijd-en-ruimte"><i class="fa fa-check"></i><b>3.13</b> Tijd en ruimte</a></li>
<li class="chapter" data-level="3.14" data-path="data.html"><a href="data.html#toegang-tot-data"><i class="fa fa-check"></i><b>3.14</b> Toegang tot data</a></li>
<li class="chapter" data-level="3.15" data-path="data.html"><a href="data.html#het-codeboek"><i class="fa fa-check"></i><b>3.15</b> Het codeboek</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data-exploratie.html"><a href="data-exploratie.html"><i class="fa fa-check"></i><b>4</b> Data exploratie</a><ul>
<li class="chapter" data-level="4.1" data-path="data-exploratie.html"><a href="data-exploratie.html#principes-van-data-exploratie"><i class="fa fa-check"></i><b>4.1</b> Principes van data exploratie</a></li>
<li class="chapter" data-level="4.2" data-path="data-exploratie.html"><a href="data-exploratie.html#stappen-in-data-exploratie"><i class="fa fa-check"></i><b>4.2</b> Stappen in data exploratie</a></li>
<li class="chapter" data-level="4.3" data-path="data-exploratie.html"><a href="data-exploratie.html#voorbeeld-data-exploratie"><i class="fa fa-check"></i><b>4.3</b> Voorbeeld data exploratie</a></li>
<li class="chapter" data-level="4.4" data-path="data-exploratie.html"><a href="data-exploratie.html#univariate-verdelingen"><i class="fa fa-check"></i><b>4.4</b> Univariate verdelingen</a></li>
<li class="chapter" data-level="4.5" data-path="data-exploratie.html"><a href="data-exploratie.html#correlatie-tussen-twee-variabelen"><i class="fa fa-check"></i><b>4.5</b> Correlatie tussen twee variabelen</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="manipuleren-van-data.html"><a href="manipuleren-van-data.html"><i class="fa fa-check"></i><b>5</b> Manipuleren van data</a><ul>
<li class="chapter" data-level="5.1" data-path="manipuleren-van-data.html"><a href="manipuleren-van-data.html#kort-overzicht-van-de-manipulaties"><i class="fa fa-check"></i><b>5.1</b> Kort overzicht van de manipulaties</a><ul>
<li class="chapter" data-level="5.1.1" data-path="manipuleren-van-data.html"><a href="manipuleren-van-data.html#filteren-en-versnijden"><i class="fa fa-check"></i><b>5.1.1</b> Filteren en versnijden</a></li>
<li class="chapter" data-level="5.1.2" data-path="manipuleren-van-data.html"><a href="manipuleren-van-data.html#booleaans-masker"><i class="fa fa-check"></i><b>5.1.2</b> Booleaans masker</a></li>
<li class="chapter" data-level="5.1.3" data-path="manipuleren-van-data.html"><a href="manipuleren-van-data.html#eng.-grouping-and-aggregation"><i class="fa fa-check"></i><b>5.1.3</b> (Eng.) Grouping and Aggregation</a></li>
<li class="chapter" data-level="5.1.4" data-path="manipuleren-van-data.html"><a href="manipuleren-van-data.html#eng.-transforming-text"><i class="fa fa-check"></i><b>5.1.4</b> (Eng.) Transforming text</a></li>
<li class="chapter" data-level="5.1.5" data-path="manipuleren-van-data.html"><a href="manipuleren-van-data.html#eng.-re-scaling-numerical-values"><i class="fa fa-check"></i><b>5.1.5</b> (Eng.) Re-Scaling Numerical Values</a></li>
<li class="chapter" data-level="5.1.6" data-path="manipuleren-van-data.html"><a href="manipuleren-van-data.html#eng.-discretizations"><i class="fa fa-check"></i><b>5.1.6</b> (Eng.) Discretizations</a></li>
<li class="chapter" data-level="5.1.7" data-path="manipuleren-van-data.html"><a href="manipuleren-van-data.html#eng.-information-content"><i class="fa fa-check"></i><b>5.1.7</b> (Eng.) Information Content</a></li>
<li class="chapter" data-level="5.1.8" data-path="manipuleren-van-data.html"><a href="manipuleren-van-data.html#eng.-reformatting-type-conversion-casting-or-coercion"><i class="fa fa-check"></i><b>5.1.8</b> (Eng.) Reformatting, Type Conversion, Casting or Coercion</a></li>
<li class="chapter" data-level="5.1.9" data-path="manipuleren-van-data.html"><a href="manipuleren-van-data.html#eng.-changing-numerical-values"><i class="fa fa-check"></i><b>5.1.9</b> (Eng.) Changing numerical Values</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="manipuleren-van-data.html"><a href="manipuleren-van-data.html#eng.-changing-category-names"><i class="fa fa-check"></i><b>5.2</b> (Eng.) Changing Category Names</a></li>
<li class="chapter" data-level="5.3" data-path="manipuleren-van-data.html"><a href="manipuleren-van-data.html#eng.-imputation"><i class="fa fa-check"></i><b>5.3</b> (Eng.) Imputation</a></li>
<li class="chapter" data-level="5.4" data-path="manipuleren-van-data.html"><a href="manipuleren-van-data.html#onbehandeld"><i class="fa fa-check"></i><b>5.4</b> Onbehandeld</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="de-percetron.html"><a href="de-percetron.html"><i class="fa fa-check"></i><b>6</b> De percetron</a><ul>
<li class="chapter" data-level="6.1" data-path="de-percetron.html"><a href="de-percetron.html#historiek"><i class="fa fa-check"></i><b>6.1</b> Historiek</a></li>
<li class="chapter" data-level="6.2" data-path="de-percetron.html"><a href="de-percetron.html#de-anatomie-van-de-perceptron"><i class="fa fa-check"></i><b>6.2</b> De anatomie van de perceptron</a></li>
<li class="chapter" data-level="6.3" data-path="de-percetron.html"><a href="de-percetron.html#casestudy-onderscheiden-van-setosa"><i class="fa fa-check"></i><b>6.3</b> Casestudy: Onderscheiden van setosa</a></li>
<li class="chapter" data-level="6.4" data-path="de-percetron.html"><a href="de-percetron.html#de-perceptron-klasse"><i class="fa fa-check"></i><b>6.4</b> De perceptron klasse</a></li>
<li class="chapter" data-level="6.5" data-path="de-percetron.html"><a href="de-percetron.html#de-perceptron-functies"><i class="fa fa-check"></i><b>6.5</b> De perceptron functies</a></li>
<li class="chapter" data-level="6.6" data-path="de-percetron.html"><a href="de-percetron.html#het-leeralgoritme-van-de-perceptron"><i class="fa fa-check"></i><b>6.6</b> Het leeralgoritme van de perceptron</a></li>
<li class="chapter" data-level="6.7" data-path="de-percetron.html"><a href="de-percetron.html#trainen-van-de-perceptron"><i class="fa fa-check"></i><b>6.7</b> Trainen van de perceptron</a></li>
<li class="chapter" data-level="6.8" data-path="de-percetron.html"><a href="de-percetron.html#voorspellen-van-de-iris-soort"><i class="fa fa-check"></i><b>6.8</b> Voorspellen van de iris soort</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="inleiding-tot-artificiële-neurale-netwerken.html"><a href="inleiding-tot-artificiële-neurale-netwerken.html"><i class="fa fa-check"></i><b>7</b> Inleiding tot Artificiële Neurale Netwerken</a><ul>
<li class="chapter" data-level="7.1" data-path="inleiding-tot-artificiële-neurale-netwerken.html"><a href="inleiding-tot-artificiële-neurale-netwerken.html#geschakelde-perceptronen"><i class="fa fa-check"></i><b>7.1</b> Geschakelde perceptronen</a></li>
<li class="chapter" data-level="7.2" data-path="inleiding-tot-artificiële-neurale-netwerken.html"><a href="inleiding-tot-artificiële-neurale-netwerken.html#feed-forward-anns-ff-anns"><i class="fa fa-check"></i><b>7.2</b> Feed-forward ANNs (FF-ANNs)</a></li>
<li class="chapter" data-level="7.3" data-path="inleiding-tot-artificiële-neurale-netwerken.html"><a href="inleiding-tot-artificiële-neurale-netwerken.html#types-neuronen"><i class="fa fa-check"></i><b>7.3</b> Types neuronen</a></li>
<li class="chapter" data-level="7.4" data-path="inleiding-tot-artificiële-neurale-netwerken.html"><a href="inleiding-tot-artificiële-neurale-netwerken.html#backpropagation"><i class="fa fa-check"></i><b>7.4</b> Backpropagation</a></li>
<li class="chapter" data-level="7.5" data-path="inleiding-tot-artificiële-neurale-netwerken.html"><a href="inleiding-tot-artificiële-neurale-netwerken.html#de-verliesfunctie-en-kost-functies"><i class="fa fa-check"></i><b>7.5</b> De verliesfunctie en kost-functies</a></li>
<li class="chapter" data-level="7.6" data-path="inleiding-tot-artificiële-neurale-netwerken.html"><a href="inleiding-tot-artificiële-neurale-netwerken.html#gradiënt-afdaling"><i class="fa fa-check"></i><b>7.6</b> Gradiënt afdaling</a></li>
<li class="chapter" data-level="7.7" data-path="inleiding-tot-artificiële-neurale-netwerken.html"><a href="inleiding-tot-artificiële-neurale-netwerken.html#stochastische-en-minibatch-gradiënt-afdaling"><i class="fa fa-check"></i><b>7.7</b> Stochastische en Minibatch gradiënt afdaling</a></li>
<li class="chapter" data-level="7.8" data-path="inleiding-tot-artificiële-neurale-netwerken.html"><a href="inleiding-tot-artificiële-neurale-netwerken.html#regularisatie"><i class="fa fa-check"></i><b>7.8</b> Regularisatie</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="convolutionele-neurale-netwerken-cnn.html"><a href="convolutionele-neurale-netwerken-cnn.html"><i class="fa fa-check"></i><b>8</b> Convolutionele Neurale Netwerken (CNN)</a><ul>
<li class="chapter" data-level="8.1" data-path="convolutionele-neurale-netwerken-cnn.html"><a href="convolutionele-neurale-netwerken-cnn.html#het-onstaan-van-computer-vision"><i class="fa fa-check"></i><b>8.1</b> Het onstaan van Computer Vision</a></li>
<li class="chapter" data-level="8.2" data-path="convolutionele-neurale-netwerken-cnn.html"><a href="convolutionele-neurale-netwerken-cnn.html#waarom-vanilla-sgd-netwerken-ontoereikend-zijn"><i class="fa fa-check"></i><b>8.2</b> Waarom vanilla SGD netwerken ontoereikend zijn</a></li>
<li class="chapter" data-level="8.3" data-path="convolutionele-neurale-netwerken-cnn.html"><a href="convolutionele-neurale-netwerken-cnn.html#de-cnn-filter"><i class="fa fa-check"></i><b>8.3</b> De CNN Filter</a></li>
<li class="chapter" data-level="8.4" data-path="convolutionele-neurale-netwerken-cnn.html"><a href="convolutionele-neurale-netwerken-cnn.html#de-filter-binnen-een-nn"><i class="fa fa-check"></i><b>8.4</b> De filter binnen een NN</a></li>
<li class="chapter" data-level="8.5" data-path="convolutionele-neurale-netwerken-cnn.html"><a href="convolutionele-neurale-netwerken-cnn.html#filter-als-compressor"><i class="fa fa-check"></i><b>8.5</b> Filter als compressor</a></li>
<li class="chapter" data-level="8.6" data-path="convolutionele-neurale-netwerken-cnn.html"><a href="convolutionele-neurale-netwerken-cnn.html#de-stride-niet-opgeven"><i class="fa fa-check"></i><b>8.6</b> De <em>stride</em> niet opgeven</a></li>
<li class="chapter" data-level="8.7" data-path="convolutionele-neurale-netwerken-cnn.html"><a href="convolutionele-neurale-netwerken-cnn.html#de-volledige-filter-laag"><i class="fa fa-check"></i><b>8.7</b> De volledige filter-laag</a></li>
<li class="chapter" data-level="8.8" data-path="convolutionele-neurale-netwerken-cnn.html"><a href="convolutionele-neurale-netwerken-cnn.html#meerdere-filters-per-laag"><i class="fa fa-check"></i><b>8.8</b> Meerdere filters per laag</a></li>
<li class="chapter" data-level="8.9" data-path="convolutionele-neurale-netwerken-cnn.html"><a href="convolutionele-neurale-netwerken-cnn.html#max-pooling"><i class="fa fa-check"></i><b>8.9</b> Max Pooling</a></li>
<li class="chapter" data-level="8.10" data-path="convolutionele-neurale-netwerken-cnn.html"><a href="convolutionele-neurale-netwerken-cnn.html#samenstellen-van-cnns"><i class="fa fa-check"></i><b>8.10</b> Samenstellen van CNNs</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="autoencoders.html"><a href="autoencoders.html"><i class="fa fa-check"></i><b>9</b> Autoencoders</a><ul>
<li class="chapter" data-level="9.1" data-path="autoencoders.html"><a href="autoencoders.html#probleemstelling-rond-dimensionaliteit-en-complexiteit"><i class="fa fa-check"></i><b>9.1</b> Probleemstelling rond Dimensionaliteit en Complexiteit</a></li>
<li class="chapter" data-level="9.2" data-path="autoencoders.html"><a href="autoencoders.html#dimensionaliteit-reduceren"><i class="fa fa-check"></i><b>9.2</b> Dimensionaliteit reduceren</a></li>
<li class="chapter" data-level="9.3" data-path="autoencoders.html"><a href="autoencoders.html#pca-om-dimensionaliteit-te-reduceren"><i class="fa fa-check"></i><b>9.3</b> PCA om dimensionaliteit te reduceren</a></li>
<li class="chapter" data-level="9.4" data-path="autoencoders.html"><a href="autoencoders.html#pca-op-de-mnist-dataset"><i class="fa fa-check"></i><b>9.4</b> PCA op de MNIST dataset</a></li>
<li class="chapter" data-level="9.5" data-path="autoencoders.html"><a href="autoencoders.html#beperkingen-van-pca"><i class="fa fa-check"></i><b>9.5</b> Beperkingen van PCA</a></li>
<li class="chapter" data-level="9.6" data-path="autoencoders.html"><a href="autoencoders.html#de-architectuur-van-de-autoencoder"><i class="fa fa-check"></i><b>9.6</b> De Architectuur van de Autoencoder</a></li>
<li class="chapter" data-level="9.7" data-path="autoencoders.html"><a href="autoencoders.html#autoencoder-op-de-mnist-dataset"><i class="fa fa-check"></i><b>9.7</b> Autoencoder op de MNIST dataset</a></li>
<li class="chapter" data-level="9.8" data-path="autoencoders.html"><a href="autoencoders.html#autoencoder-met-cnn"><i class="fa fa-check"></i><b>9.8</b> Autoencoder met CNN</a></li>
<li class="chapter" data-level="9.9" data-path="autoencoders.html"><a href="autoencoders.html#autoencoder-als-ruis-verwijderaar"><i class="fa fa-check"></i><b>9.9</b> Autoencoder als ruis-verwijderaar</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="trainen-en-testen.html"><a href="trainen-en-testen.html"><i class="fa fa-check"></i><b>10</b> Trainen en testen</a><ul>
<li class="chapter" data-level="10.1" data-path="trainen-en-testen.html"><a href="trainen-en-testen.html#leren-leven-met-de-onzekerheid"><i class="fa fa-check"></i><b>10.1</b> Leren leven met de onzekerheid</a></li>
<li class="chapter" data-level="10.2" data-path="trainen-en-testen.html"><a href="trainen-en-testen.html#meten-van-de-prestatie-van-een-model"><i class="fa fa-check"></i><b>10.2</b> Meten van de prestatie van een model</a></li>
<li class="chapter" data-level="10.3" data-path="trainen-en-testen.html"><a href="trainen-en-testen.html#training--validatie--en-test-set"><i class="fa fa-check"></i><b>10.3</b> Training-, validatie- en test-set</a></li>
<li class="chapter" data-level="10.4" data-path="trainen-en-testen.html"><a href="trainen-en-testen.html#cross-validatie"><i class="fa fa-check"></i><b>10.4</b> Cross-validatie</a></li>
<li class="chapter" data-level="10.5" data-path="trainen-en-testen.html"><a href="trainen-en-testen.html#werkstroom-deep-learning"><i class="fa fa-check"></i><b>10.5</b> Werkstroom deep learning</a></li>
<li class="chapter" data-level="10.6" data-path="trainen-en-testen.html"><a href="trainen-en-testen.html#data-lekkage"><i class="fa fa-check"></i><b>10.6</b> Data lekkage</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="sequentie-analyse.html"><a href="sequentie-analyse.html"><i class="fa fa-check"></i><b>11</b> Sequentie Analyse</a><ul>
<li class="chapter" data-level="11.1" data-path="sequentie-analyse.html"><a href="sequentie-analyse.html#inleiding-tot-sequentie-analyse"><i class="fa fa-check"></i><b>11.1</b> Inleiding tot sequentie analyse</a></li>
<li class="chapter" data-level="11.2" data-path="sequentie-analyse.html"><a href="sequentie-analyse.html#sequence-to-sequence"><i class="fa fa-check"></i><b>11.2</b> Sequence-To-Sequence</a></li>
<li class="chapter" data-level="11.3" data-path="sequentie-analyse.html"><a href="sequentie-analyse.html#recurrente-nn"><i class="fa fa-check"></i><b>11.3</b> Recurrente NN</a></li>
<li class="chapter" data-level="11.4" data-path="sequentie-analyse.html"><a href="sequentie-analyse.html#verdwijnende-gradiënten"><i class="fa fa-check"></i><b>11.4</b> Verdwijnende gradiënten</a></li>
<li class="chapter" data-level="11.5" data-path="sequentie-analyse.html"><a href="sequentie-analyse.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>11.5</b> Long short-term memory (LSTM)</a></li>
<li class="chapter" data-level="11.6" data-path="sequentie-analyse.html"><a href="sequentie-analyse.html#sentiment-analyse"><i class="fa fa-check"></i><b>11.6</b> Sentiment analyse</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="neurale-netwerken-met-extern-geheugen.html"><a href="neurale-netwerken-met-extern-geheugen.html"><i class="fa fa-check"></i><b>12</b> Neurale Netwerken met Extern Geheugen</a><ul>
<li class="chapter" data-level="12.1" data-path="neurale-netwerken-met-extern-geheugen.html"><a href="neurale-netwerken-met-extern-geheugen.html#inleiding-nn-met-extern-geheugen"><i class="fa fa-check"></i><b>12.1</b> Inleiding NN met extern geheugen</a></li>
<li class="chapter" data-level="12.2" data-path="neurale-netwerken-met-extern-geheugen.html"><a href="neurale-netwerken-met-extern-geheugen.html#neurale-turing-machines"><i class="fa fa-check"></i><b>12.2</b> Neurale Turing Machines</a></li>
<li class="chapter" data-level="12.3" data-path="neurale-netwerken-met-extern-geheugen.html"><a href="neurale-netwerken-met-extern-geheugen.html#lezen-uit-en-schrijven-naar-een-ntm-geheugen"><i class="fa fa-check"></i><b>12.3</b> Lezen uit en schrijven naar een NTM geheugen</a></li>
<li class="chapter" data-level="12.4" data-path="neurale-netwerken-met-extern-geheugen.html"><a href="neurale-netwerken-met-extern-geheugen.html#adressering-van-ntm-geheugens"><i class="fa fa-check"></i><b>12.4</b> Adressering van NTM geheugens</a></li>
<li class="chapter" data-level="12.5" data-path="neurale-netwerken-met-extern-geheugen.html"><a href="neurale-netwerken-met-extern-geheugen.html#inhoud-gebaseerde-adressering"><i class="fa fa-check"></i><b>12.5</b> Inhoud-gebaseerde adressering</a></li>
<li class="chapter" data-level="12.6" data-path="neurale-netwerken-met-extern-geheugen.html"><a href="neurale-netwerken-met-extern-geheugen.html#locatie-gebaseerde-adressering"><i class="fa fa-check"></i><b>12.6</b> Locatie-gebaseerde adressering</a></li>
<li class="chapter" data-level="12.7" data-path="neurale-netwerken-met-extern-geheugen.html"><a href="neurale-netwerken-met-extern-geheugen.html#adresseringsmechanisme"><i class="fa fa-check"></i><b>12.7</b> Adresseringsmechanisme</a></li>
<li class="chapter" data-level="12.8" data-path="neurale-netwerken-met-extern-geheugen.html"><a href="neurale-netwerken-met-extern-geheugen.html#de-nadelen-van-ntms"><i class="fa fa-check"></i><b>12.8</b> De nadelen van NTMs</a></li>
<li class="chapter" data-level="12.9" data-path="neurale-netwerken-met-extern-geheugen.html"><a href="neurale-netwerken-met-extern-geheugen.html#de-differentiële-neurale-computer"><i class="fa fa-check"></i><b>12.9</b> De differentiële neurale computer</a></li>
<li class="chapter" data-level="12.10" data-path="neurale-netwerken-met-extern-geheugen.html"><a href="neurale-netwerken-met-extern-geheugen.html#implementatie-dnc"><i class="fa fa-check"></i><b>12.10</b> Implementatie DNC</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="deep-reinforcement-learning.html"><a href="deep-reinforcement-learning.html"><i class="fa fa-check"></i><b>13</b> Deep Reinforcement Learning</a><ul>
<li class="chapter" data-level="13.1" data-path="deep-reinforcement-learning.html"><a href="deep-reinforcement-learning.html#inleiding-deep-rl"><i class="fa fa-check"></i><b>13.1</b> Inleiding Deep RL</a></li>
<li class="chapter" data-level="13.2" data-path="deep-reinforcement-learning.html"><a href="deep-reinforcement-learning.html#voorbeelden-van-deep-reinforcement-learning"><i class="fa fa-check"></i><b>13.2</b> Voorbeelden van deep reinforcement learning</a><ul>
<li class="chapter" data-level="13.2.1" data-path="deep-reinforcement-learning.html"><a href="deep-reinforcement-learning.html#introductie-deepmind-team"><i class="fa fa-check"></i><b>13.2.1</b> Introductie DeepMind Team</a></li>
<li class="chapter" data-level="13.2.2" data-path="deep-reinforcement-learning.html"><a href="deep-reinforcement-learning.html#deepminds-deep-q-learning"><i class="fa fa-check"></i><b>13.2.2</b> DeepMind’s Deep-Q learning</a></li>
<li class="chapter" data-level="13.2.3" data-path="deep-reinforcement-learning.html"><a href="deep-reinforcement-learning.html#robot-tasks"><i class="fa fa-check"></i><b>13.2.3</b> Robot tasks</a></li>
<li class="chapter" data-level="13.2.4" data-path="deep-reinforcement-learning.html"><a href="deep-reinforcement-learning.html#atlas"><i class="fa fa-check"></i><b>13.2.4</b> Atlas</a></li>
<li class="chapter" data-level="13.2.5" data-path="deep-reinforcement-learning.html"><a href="deep-reinforcement-learning.html#cart-pole"><i class="fa fa-check"></i><b>13.2.5</b> Cart-Pole</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="deep-reinforcement-learning.html"><a href="deep-reinforcement-learning.html#markov-beslissingsproces"><i class="fa fa-check"></i><b>13.3</b> Markov beslissingsproces</a></li>
<li class="chapter" data-level="13.4" data-path="deep-reinforcement-learning.html"><a href="deep-reinforcement-learning.html#deep-reinforcement-learning-1"><i class="fa fa-check"></i><b>13.4</b> Deep Reinforcement Learning</a></li>
<li class="chapter" data-level="13.5" data-path="deep-reinforcement-learning.html"><a href="deep-reinforcement-learning.html#varianten-van-deep-rl"><i class="fa fa-check"></i><b>13.5</b> Varianten van (Deep) RL</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="rapporteren.html"><a href="rapporteren.html"><i class="fa fa-check"></i><b>14</b> Rapporteren</a><ul>
<li class="chapter" data-level="14.1" data-path="rapporteren.html"><a href="rapporteren.html#vormen-van-schriftelijke-communicatie"><i class="fa fa-check"></i><b>14.1</b> Vormen van schriftelijke communicatie</a></li>
<li class="chapter" data-level="14.2" data-path="rapporteren.html"><a href="rapporteren.html#de-vraagstelling"><i class="fa fa-check"></i><b>14.2</b> De vraagstelling</a></li>
<li class="chapter" data-level="14.3" data-path="rapporteren.html"><a href="rapporteren.html#de-probleemstelling"><i class="fa fa-check"></i><b>14.3</b> De probleemstelling</a></li>
<li class="chapter" data-level="14.4" data-path="rapporteren.html"><a href="rapporteren.html#uitvoering-ai-project"><i class="fa fa-check"></i><b>14.4</b> Uitvoering AI project</a><ul>
<li class="chapter" data-level="14.4.1" data-path="rapporteren.html"><a href="rapporteren.html#reproduceerbare-willekeur"><i class="fa fa-check"></i><b>14.4.1</b> Reproduceerbare willekeur</a></li>
<li class="chapter" data-level="14.4.2" data-path="rapporteren.html"><a href="rapporteren.html#tools"><i class="fa fa-check"></i><b>14.4.2</b> Tools</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="rapporteren.html"><a href="rapporteren.html#de-inleiding-van-een-rapport"><i class="fa fa-check"></i><b>14.5</b> De inleiding van een rapport</a></li>
<li class="chapter" data-level="14.6" data-path="rapporteren.html"><a href="rapporteren.html#methodiek"><i class="fa fa-check"></i><b>14.6</b> Methodiek</a><ul>
<li class="chapter" data-level="14.6.1" data-path="rapporteren.html"><a href="rapporteren.html#data-beschikbaar-maken"><i class="fa fa-check"></i><b>14.6.1</b> Data beschikbaar maken</a></li>
<li class="chapter" data-level="14.6.2" data-path="rapporteren.html"><a href="rapporteren.html#beschikbaar-maken-van-databanken"><i class="fa fa-check"></i><b>14.6.2</b> Beschikbaar maken van databanken</a></li>
<li class="chapter" data-level="14.6.3" data-path="rapporteren.html"><a href="rapporteren.html#procesbeschrijving"><i class="fa fa-check"></i><b>14.6.3</b> Procesbeschrijving</a></li>
<li class="chapter" data-level="14.6.4" data-path="rapporteren.html"><a href="rapporteren.html#voorbeeld-uit-wang-et-al."><i class="fa fa-check"></i><b>14.6.4</b> Voorbeeld uit Wang et al.</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="rapporteren.html"><a href="rapporteren.html#resultaten"><i class="fa fa-check"></i><b>14.7</b> Resultaten</a><ul>
<li class="chapter" data-level="14.7.1" data-path="rapporteren.html"><a href="rapporteren.html#beduidende-cijfers"><i class="fa fa-check"></i><b>14.7.1</b> Beduidende cijfers</a></li>
<li class="chapter" data-level="14.7.2" data-path="rapporteren.html"><a href="rapporteren.html#onzekere-cijfers"><i class="fa fa-check"></i><b>14.7.2</b> Onzekere cijfers</a></li>
<li class="chapter" data-level="14.7.3" data-path="rapporteren.html"><a href="rapporteren.html#visuele-cijfers"><i class="fa fa-check"></i><b>14.7.3</b> Visuele cijfers</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="rapporteren.html"><a href="rapporteren.html#discussie-en-conclusie"><i class="fa fa-check"></i><b>14.8</b> Discussie en Conclusie</a></li>
<li class="chapter" data-level="14.9" data-path="rapporteren.html"><a href="rapporteren.html#afsluiten-met-de-samenvatting"><i class="fa fa-check"></i><b>14.9</b> Afsluiten met de samenvatting</a></li>
<li class="chapter" data-level="14.10" data-path="rapporteren.html"><a href="rapporteren.html#verwijzen-naar-extern-werk"><i class="fa fa-check"></i><b>14.10</b> Verwijzen naar extern werk</a><ul>
<li class="chapter" data-level="14.10.1" data-path="rapporteren.html"><a href="rapporteren.html#citeren"><i class="fa fa-check"></i><b>14.10.1</b> Citeren</a></li>
<li class="chapter" data-level="14.10.2" data-path="rapporteren.html"><a href="rapporteren.html#licenties-en-toestemming"><i class="fa fa-check"></i><b>14.10.2</b> Licenties en toestemming</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ethisch-ml.html"><a href="ethisch-ml.html"><i class="fa fa-check"></i><b>15</b> Ethisch ML</a><ul>
<li class="chapter" data-level="15.1" data-path="ethisch-ml.html"><a href="ethisch-ml.html#inleiding-tot-de-ml-ethiek"><i class="fa fa-check"></i><b>15.1</b> Inleiding tot de ML-ethiek</a></li>
<li class="chapter" data-level="15.2" data-path="ethisch-ml.html"><a href="ethisch-ml.html#hoe-het-niet-moet"><i class="fa fa-check"></i><b>15.2</b> Hoe het niet moet</a><ul>
<li class="chapter" data-level="15.2.1" data-path="ethisch-ml.html"><a href="ethisch-ml.html#gender-ongelijkheid"><i class="fa fa-check"></i><b>15.2.1</b> Gender-ongelijkheid</a></li>
<li class="chapter" data-level="15.2.2" data-path="ethisch-ml.html"><a href="ethisch-ml.html#onmenselijk"><i class="fa fa-check"></i><b>15.2.2</b> Onmenselijk</a></li>
<li class="chapter" data-level="15.2.3" data-path="ethisch-ml.html"><a href="ethisch-ml.html#vals-gevoel-van-controle"><i class="fa fa-check"></i><b>15.2.3</b> Vals gevoel van controle</a></li>
<li class="chapter" data-level="15.2.4" data-path="ethisch-ml.html"><a href="ethisch-ml.html#gamification"><i class="fa fa-check"></i><b>15.2.4</b> Gamification</a></li>
<li class="chapter" data-level="15.2.5" data-path="ethisch-ml.html"><a href="ethisch-ml.html#ongewilde-advertenties"><i class="fa fa-check"></i><b>15.2.5</b> Ongewilde advertenties</a></li>
<li class="chapter" data-level="15.2.6" data-path="ethisch-ml.html"><a href="ethisch-ml.html#huidskleur"><i class="fa fa-check"></i><b>15.2.6</b> Huidskleur</a></li>
<li class="chapter" data-level="15.2.7" data-path="ethisch-ml.html"><a href="ethisch-ml.html#polarisatie"><i class="fa fa-check"></i><b>15.2.7</b> Polarisatie</a></li>
<li class="chapter" data-level="15.2.8" data-path="ethisch-ml.html"><a href="ethisch-ml.html#gezondheid"><i class="fa fa-check"></i><b>15.2.8</b> Gezondheid</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ethisch-ml.html"><a href="ethisch-ml.html#de-oorzaken-van-onethisch-ai-producten"><i class="fa fa-check"></i><b>15.3</b> De oorzaken van onethisch AI-producten</a></li>
<li class="chapter" data-level="15.4" data-path="ethisch-ml.html"><a href="ethisch-ml.html#representativiteit"><i class="fa fa-check"></i><b>15.4</b> Representativiteit</a></li>
<li class="chapter" data-level="15.5" data-path="ethisch-ml.html"><a href="ethisch-ml.html#randvoorwaarden"><i class="fa fa-check"></i><b>15.5</b> Randvoorwaarden</a></li>
<li class="chapter" data-level="15.6" data-path="ethisch-ml.html"><a href="ethisch-ml.html#privacy-en-ethiek"><i class="fa fa-check"></i><b>15.6</b> Privacy en ethiek</a></li>
<li class="chapter" data-level="15.7" data-path="ethisch-ml.html"><a href="ethisch-ml.html#privacy"><i class="fa fa-check"></i><b>15.7</b> Privacy</a></li>
<li class="chapter" data-level="15.8" data-path="ethisch-ml.html"><a href="ethisch-ml.html#de-drie-wetten-van-asimov"><i class="fa fa-check"></i><b>15.8</b> De drie wetten van Asimov</a></li>
<li class="chapter" data-level="15.9" data-path="ethisch-ml.html"><a href="ethisch-ml.html#ethiek"><i class="fa fa-check"></i><b>15.9</b> Ethiek</a></li>
<li class="chapter" data-level="15.10" data-path="ethisch-ml.html"><a href="ethisch-ml.html#proces-om-etisch-te-blijven"><i class="fa fa-check"></i><b>15.10</b> Proces om etisch te blijven</a></li>
<li class="chapter" data-level="15.11" data-path="ethisch-ml.html"><a href="ethisch-ml.html#regels-rond-ethiek"><i class="fa fa-check"></i><b>15.11</b> Regels rond ethiek</a></li>
<li class="chapter" data-level="15.12" data-path="ethisch-ml.html"><a href="ethisch-ml.html#eed"><i class="fa fa-check"></i><b>15.12</b> Eed</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="de-logaritme.html"><a href="de-logaritme.html"><i class="fa fa-check"></i><b>A</b> De Logaritme</a></li>
<li class="chapter" data-level="B" data-path="normaliseren-versus-standaardiseren.html"><a href="normaliseren-versus-standaardiseren.html"><i class="fa fa-check"></i><b>B</b> Normaliseren versus Standaardiseren</a></li>
<li class="chapter" data-level="C" data-path="inwendig-product-matrix-vermenigvuldiging-vectoren-en-tensoren.html"><a href="inwendig-product-matrix-vermenigvuldiging-vectoren-en-tensoren.html"><i class="fa fa-check"></i><b>C</b> Inwendig product, matrix-vermenigvuldiging, vectoren en tensoren</a></li>
<li class="chapter" data-level="D" data-path="computations-using-gpu.html"><a href="computations-using-gpu.html"><i class="fa fa-check"></i><b>D</b> Computations using GPU</a></li>
<li class="chapter" data-level="E" data-path="installation.html"><a href="installation.html"><i class="fa fa-check"></i><b>E</b> Installation</a><ul>
<li class="chapter" data-level="E.1" data-path="installation.html"><a href="installation.html#installing-cuda-optional"><i class="fa fa-check"></i><b>E.1</b> Installing CUDA (optional)</a></li>
<li class="chapter" data-level="E.2" data-path="installation.html"><a href="installation.html#the-r-language"><i class="fa fa-check"></i><b>E.2</b> The R language</a></li>
<li class="chapter" data-level="E.3" data-path="installation.html"><a href="installation.html#python"><i class="fa fa-check"></i><b>E.3</b> Python</a></li>
<li class="chapter" data-level="E.4" data-path="installation.html"><a href="installation.html#rstudio"><i class="fa fa-check"></i><b>E.4</b> RStudio</a></li>
<li class="chapter" data-level="E.5" data-path="installation.html"><a href="installation.html#installing-tensorflow"><i class="fa fa-check"></i><b>E.5</b> Installing Tensorflow</a></li>
<li class="chapter" data-level="E.6" data-path="installation.html"><a href="installation.html#installation-steps-that-worked-for-the-author"><i class="fa fa-check"></i><b>E.6</b> Installation steps that worked for the author</a></li>
<li class="chapter" data-level="E.7" data-path="installation.html"><a href="installation.html#waar-vind-ik-hulp"><i class="fa fa-check"></i><b>E.7</b> Waar vind ik hulp</a></li>
<li class="chapter" data-level="E.8" data-path="installation.html"><a href="installation.html#waar-kan-ik-leeralgoritmen-terugvinden"><i class="fa fa-check"></i><b>E.8</b> Waar kan ik leeralgoritmen terugvinden</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="git.html"><a href="git.html"><i class="fa fa-check"></i><b>F</b> Git</a></li>
<li class="chapter" data-level="G" data-path="antwoorden.html"><a href="antwoorden.html"><i class="fa fa-check"></i><b>G</b> Antwoorden</a><ul>
<li class="chapter" data-level="G.1" data-path="antwoorden.html"><a href="antwoorden.html#hoofdstuk-4"><i class="fa fa-check"></i><b>G.1</b> Hoofdstuk 4</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bronvermelding.html"><a href="bronvermelding.html"><i class="fa fa-check"></i>Bronvermelding</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="de-percetron" class="section level1">
<h1><span class="header-section-number">Hoofdstuk 6</span> De percetron</h1>
<div id="historiek" class="section level2">
<h2><span class="header-section-number">6.1</span> Historiek</h2>
<p>De perceptron is gebaseerde op een wiskundige beschrijving van het dierlijk neuron. De uitvinding van de perceptron wordt toegekend aan <a href="https://en.wikipedia.org/wiki/Frank_Rosenblatt">Frank Rosenblatt</a> die in 1957 een leeralgoritme programmeerde op een IBM 704 computer.</p>
<div class="figure"><span id="fig:ibm-704"></span>
<img src="https://upload.wikimedia.org/wikipedia/commons/7/7d/IBM_704_mainframe.gif" alt="IBM 704 mainframe. bron: Lawrence Livermore National Laboratory n.d."  />
<p class="caption">
Figuur 6.1: IBM 704 mainframe. bron: <span class="citation">Lawrence Livermore National Laboratory <a href="#ref-ibm-704" role="doc-biblioref">n.d.</a></span>
</p>
</div>

<p>In de <a href="https://www.nytimes.com/1958/07/08/archives/new-navy-device-learns-by-doing-psychologist-shows-embryo-of.html">New York Times van 8 Juli 1958</a> verscheen:</p>
<p><q>The Navy revealed the embryo of an electronic computer today that it expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.</q></p>
<p>Al snel werd er hardware gemaakt om het algoritme efficiënt uit te voeren, de zogenaamde Mark I perceptron machine</p>
<div class="figure"><span id="fig:mark-1"></span>
<img src="https://upload.wikimedia.org/wikipedia/en/5/52/Mark_I_perceptron.jpeg" alt="De Mark I perceptron machine, waarbij een matrix van 20 bij 20 cadmium sulfide fotocellen verbonden werden met een reeks potentiometers die de gewichten voorzien van de 400 invoer noden. bron: Cornell University Library n.d."  />
<p class="caption">
Figuur 6.2: De Mark I perceptron machine, waarbij een matrix van 20 bij 20 cadmium sulfide fotocellen verbonden werden met een reeks potentiometers die de gewichten voorzien van de 400 invoer noden. bron: <span class="citation">Cornell University Library <a href="#ref-mark-1" role="doc-biblioref">n.d.</a></span>
</p>
</div>

</div>
<div id="de-anatomie-van-de-perceptron" class="section level2">
<h2><span class="header-section-number">6.2</span> De anatomie van de perceptron</h2>
<div class="figure"><span id="fig:neuron"></span>
<img src="img/neuron.png" alt="De anatomie van het neuron en de overeenkomstige node van een neuraal netwerk. In het dierlijke neuron (boven) krijgt het cellichaam (eng: cell body) allerlei invoer van verscheidene locaties. In dit cellichaam vindt de integratie plaats (sommatie) en transformatie waarna het signaal (in de vorm van een zogenaamde actiepotentiaal) langsheen het axon reist naar het axon-uiteinde (eng: synaptic terminals). In een neuraal netwerk zijn de axon-uiteinden verbonden met de dendrieten (eng: dendrites) van een ander neuron. In een perceptron (onder) wordt de invoer (\(x\)) gewogen en geïntegreerd tot één getal (\(z\)) dat getransformeerd kan worden alvorens het wordt als uitvoer (\(y\)) wordt vrijgegeven. Bron: Commons 2013." width="409" />
<p class="caption">
Figuur 6.3: De anatomie van het neuron en de overeenkomstige node van een neuraal netwerk. In het dierlijke neuron (boven) krijgt het cellichaam (eng: <em>cell body</em>) allerlei invoer van verscheidene locaties. In dit cellichaam vindt de integratie plaats (sommatie) en transformatie waarna het signaal (in de vorm van een zogenaamde actiepotentiaal) langsheen het axon reist naar het axon-uiteinde (eng: <em>synaptic terminals</em>). In een neuraal netwerk zijn de axon-uiteinden verbonden met de dendrieten (eng: <em>dendrites</em>) van een ander neuron. In een perceptron (onder) wordt de invoer (<span class="math inline">\(x\)</span>) gewogen en geïntegreerd tot één getal (<span class="math inline">\(z\)</span>) dat getransformeerd kan worden alvorens het wordt als uitvoer (<span class="math inline">\(y\)</span>) wordt vrijgegeven. Bron: <span class="citation">Commons <a href="#ref-neuron" role="doc-biblioref">2013</a></span>.
</p>
</div>

<p>Naar analogie met het biologisch neuron is de functie van een perceptron om complexe invoer om de ene of andere manier te reduceren tot één getal (<span class="math inline">\(z\)</span>; zie Formule <a href="de-percetron.html#eq:perceptron">(6.1)</a>). Dit getal wordt soms de <em>logit</em> van een neuron genoemd.</p>
<p><span class="math display" id="eq:perceptron">\[\begin{equation}
	\mathbb{R}^n\rightarrow\mathbb{R}\\
  z=g(x, \theta)=\sum_{i=0}^{n}{\theta_ix_i}
  \tag{6.1}
\end{equation}\]</span></p>
<p>In een tweede functie, de zogenaamde <em>activatiefunctie</em> (eng: <em>activation function</em>), wordt dit resultaat nog eens getransformeerd tot de voorspelling <span class="math inline">\(\hat{y}\)</span>. Één voorbeeld van zulk een transformatie is <a href="https://nl.wikipedia.org/wiki/Signum_(wiskunde)">het signum</a> (eng:<em>sign function</em>):</p>
<p><span class="math display" id="eq:perceptron-transformatie">\[\begin{equation}
	\hat{y}=t(z)=
	\begin{cases}
	-1\,als\,z&lt;0\\
	1\,als\,z\geq0
	\end{cases}
  \tag{6.2}
\end{equation}\]</span></p>
<p>Er zijn tal van andere transformaties mogelijk.zoals de <a href="https://nl.wikipedia.org/wiki/Heaviside-functie">heaviside-functie</a> die nul geeft indien <span class="math inline">\(z&lt;0\)</span> en 1 in alle andere gevallen.</p>
</div>
<div id="casestudy-onderscheiden-van-setosa" class="section level2">
<h2><span class="header-section-number">6.3</span> Casestudy: Onderscheiden van setosa</h2>
<p>De beste manier vaak om iets te begrijpen is om de code te zien, toch? Op basis van een vereenvoudigd voorbeeld gebaseerd op de blog van Simone Alberto Peirone (<span class="citation">Peirone <a href="#ref-perceptron-scratch" role="doc-biblioref">2007</a></span>) gaan we een leeralgoritme uit het niets opbouwen. Dit kan hier alleen omdat het om een uiterst eenvoudig leeralgoritme gaat, de perceptron.</p>
<p>In de beroemde iris dataset worden een aantal eigenschappen van drie sterk gelijkende iris soorten verzameld. We kwamen deze dataset eerder al tegen in het hoofdstuk rond <a href="#model-coplexiteit">model complexiteit</a>.</p>
<div class="figure"><span id="fig:iris"></span>
<img src="img/iris.png" alt="De drie soort van de iris dataset. Bron: Szczecinkowaty 2007, Mayfield 2005, Mayfield 2007." width="440" />
<p class="caption">
Figuur 6.4: De drie soort van de iris dataset. Bron: <span class="citation">Szczecinkowaty <a href="#ref-iris-setosa" role="doc-biblioref">2007</a></span>, <span class="citation">Mayfield <a href="#ref-iris-versicolor" role="doc-biblioref">2005</a></span>, <span class="citation">Mayfield <a href="#ref-iris-virginica" role="doc-biblioref">2007</a></span>.
</p>
</div>

<p>Als we twee eigenschappen van de iris bloemen, namelijk de lengte (<span class="math inline">\(l\)</span>) en breedte (<span class="math inline">\(b\)</span>) van het kelkblad, tegenover elkaar uitzetten, dan kunnen we de setosa soort van de andere twee onderscheiden:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="de-percetron.html#cb57-1"></a><span class="kw">data</span>(iris)</span>
<span id="cb57-2"><a href="de-percetron.html#cb57-2"></a></span>
<span id="cb57-3"><a href="de-percetron.html#cb57-3"></a><span class="kw">plot</span>(Sepal.Length <span class="op">~</span><span class="st"> </span>Sepal.Width, <span class="dt">data =</span> iris, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">cex =</span> <span class="fl">.8</span>,</span>
<span id="cb57-4"><a href="de-percetron.html#cb57-4"></a>	<span class="dt">col =</span> <span class="dv">1</span> <span class="op">+</span><span class="st"> </span>(Species <span class="op">==</span><span class="st"> &quot;setosa&quot;</span>),</span>
<span id="cb57-5"><a href="de-percetron.html#cb57-5"></a>	 <span class="dt">xlab =</span> <span class="kw">expression</span>(<span class="st">&quot;Kelkblad breedte &quot;</span><span class="op">~</span><span class="kw">italic</span>(b)<span class="op">~</span><span class="st">&quot; (mm)&quot;</span>),</span>
<span id="cb57-6"><a href="de-percetron.html#cb57-6"></a>	 <span class="dt">ylab =</span> <span class="kw">expression</span>(<span class="st">&quot;Kelkblad lengte &quot;</span><span class="op">~</span><span class="kw">italic</span>(l)<span class="op">~</span><span class="st">&quot; (mm)&quot;</span>),</span>
<span id="cb57-7"><a href="de-percetron.html#cb57-7"></a>	<span class="dt">main =</span> <span class="st">&quot;Setosa vs {versicolor, virginica}&quot;</span>)</span>
<span id="cb57-8"><a href="de-percetron.html#cb57-8"></a></span>
<span id="cb57-9"><a href="de-percetron.html#cb57-9"></a><span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;{versicolor, virginica}&quot;</span>, <span class="st">&quot;setosa&quot;</span>),</span>
<span id="cb57-10"><a href="de-percetron.html#cb57-10"></a>  <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="dt">pch =</span> <span class="dv">19</span>)</span></code></pre></div>
<div class="figure"><span id="fig:iris-scheiden"></span>
<img src="_main_files/figure-html/iris-scheiden-1.png" alt="Twee eigenschappen van iris bloemen (kelkblad lengte en breedte) tegenover elkaar uitgezet om een onderscheid te maken tussen Iris setosa en de twee andere soorten Iris versicolor en Iris virginica." width="672" />
<p class="caption">
Figuur 6.5: Twee eigenschappen van iris bloemen (kelkblad lengte en breedte) tegenover elkaar uitgezet om een onderscheid te maken tussen <em>Iris setosa</em> en de twee andere soorten <em>Iris versicolor</em> en <em>Iris virginica</em>.
</p>
</div>

<p>In dit twee-dimensionaal geval zouden we dat evengoed manueel kunnen doen, maar dit geldt niet voor hogere dimensies (i.e. méér variabelen). Daarom gaan we alsnog gebruik maken van een perceptron. We doen dit omdat het perceptron natuurlijk de eenvoudigste vorm van een neuraal netwerk is, maar weet dat een perceptron slechts in een beperkt aantal gevallen geschikt is.</p>

<div class="definition">
<span id="def:def-perceptron" class="definition"><strong>Stelling 6.1  </strong></span>De perceptron is een leeralgoritme dat enkel geschikt is voor het onderscheiden van twee klassen (binaire outcome) en indien beide klassen lineair onderscheidbaar zijn (eng: <em>linearly separable</em>).
</div>

</div>
<div id="de-perceptron-klasse" class="section level2">
<h2><span class="header-section-number">6.4</span> De perceptron klasse</h2>
<p>We gaan de perceptron in Python coderen, terwijl de resultaten in R zullen worden onderzocht (zie de § <em>The R Language</em> in de Appendix om te begrijpen waarom).</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="de-percetron.html#cb58-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb58-2"><a href="de-percetron.html#cb58-2"></a></span>
<span id="cb58-3"><a href="de-percetron.html#cb58-3"></a><span class="kw">class</span> Perceptron:</span>
<span id="cb58-4"><a href="de-percetron.html#cb58-4"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, learning_rate <span class="op">=</span> <span class="fl">0.1</span>, iteration_count <span class="op">=</span> <span class="dv">50</span>):</span>
<span id="cb58-5"><a href="de-percetron.html#cb58-5"></a>    <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb58-6"><a href="de-percetron.html#cb58-6"></a>    <span class="va">self</span>.iteration_count <span class="op">=</span> iteration_count</span>
<span id="cb58-7"><a href="de-percetron.html#cb58-7"></a>    <span class="va">self</span>.theta_0 <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb58-8"><a href="de-percetron.html#cb58-8"></a>    <span class="va">self</span>.theta <span class="op">=</span> <span class="va">None</span></span>
<span id="cb58-9"><a href="de-percetron.html#cb58-9"></a>    <span class="va">self</span>.errs <span class="op">=</span> []</span></code></pre></div>
<p>De <code>Perceptron</code> klasse wordt geïnitialiseerd met de hyperparameters <code>learning_rate</code> en <code>n-iter</code>. De <code>learning_rate</code> (nl: <em>leersnelheid</em>) is een correctiefactor waarmee de gewichten <span class="math inline">\(\theta_i\)</span> worden vermenigvuldigd indien de uitkomst van de instantie fout is. <code>iteration_count</code> geeft aan hoeveel cycli het leeralgoritme moet volbrengen. <code>theta_0</code> en <code>theta</code> zijn de <em>parameters</em> en <code>theta_0</code> is een constante <span class="math inline">\(\theta_0\)</span> die aan de formule <a href="de-percetron.html#eq:perceptron">(6.1)</a>) kan worden toegevoegd (zie Formule <a href="de-percetron.html#eq:perceptron-constante">(6.3)</a> en komt in het tweedimensionaal geval neer op een asafsnede (eng: <em>intercept</em>). Deze constante wordt echter meestal weggelaten omdat het erop neerkomt dat een extra berekende variabele <span class="math inline">\(x_0\)</span> wordt toegevoegd, bestaande uit allemaal enen, waarvoor dan een overeenkomstige <span class="math inline">\(\theta_0\)</span> wordt voorzien.</p>
<p><span class="math display" id="eq:perceptron-constante">\[\begin{equation}
  g(x,\theta)=\theta_0+\sum_{i=1}^{n}{\theta_ix_i}
  \tag{6.3}
\end{equation}\]</span></p>
<p>In de interne variabele <code>errs</code> worden het aantal fouten bewaard die bij elke cyclus gemaakt worden. Hiermee kunnen we het verloop van de performantie van het leeralgoritme opvolgen of achteraf reconstrueren.</p>
<p>Hoewel in bovenstaande code hier nog geen gebruik van wordt gemaakt, zorgt de geïmporteerde <a href="https://numpy.org/"><code>numpy</code> module</a> (onder alias <code>np</code>) ervoor dat er vlot met vectoren en matrices gerekend kan worden.</p>
</div>
<div id="de-perceptron-functies" class="section level2">
<h2><span class="header-section-number">6.5</span> De perceptron functies</h2>
<p>We zagen twee functies die de eigenlijke kern vormen van de perceptron: <span class="math inline">\(g\)</span> en <span class="math inline">\(t\)</span>. Laten we deze als <a href="https://stackoverflow.com/questions/155609/whats-the-difference-between-a-method-and-a-function">methoden</a> toevoegen:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="de-percetron.html#cb59-1"></a><span class="kw">class</span> Perceptron:</span>
<span id="cb59-2"><a href="de-percetron.html#cb59-2"></a>  [...]</span>
<span id="cb59-3"><a href="de-percetron.html#cb59-3"></a></span>
<span id="cb59-4"><a href="de-percetron.html#cb59-4"></a>  <span class="kw">def</span> g(<span class="va">self</span>, x: np.array) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb59-5"><a href="de-percetron.html#cb59-5"></a>    <span class="cf">return</span> np.dot(x, <span class="va">self</span>.theta) <span class="op">+</span> <span class="va">self</span>.theta_0</span>
<span id="cb59-6"><a href="de-percetron.html#cb59-6"></a>  </span>
<span id="cb59-7"><a href="de-percetron.html#cb59-7"></a>  <span class="kw">def</span> t(<span class="va">self</span>, z: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb59-8"><a href="de-percetron.html#cb59-8"></a>    <span class="cf">return</span> np.where(z <span class="op">&gt;=</span> <span class="fl">0.0</span>, <span class="dv">1</span>, <span class="dv">-1</span>)</span></code></pre></div>
<p>Met de <a href="https://numpy.org/doc/stable/reference/generated/numpy.dot.html?highlight=dot#numpy.dot"><code>np.dot</code></a> functie berekenen we het <a href="https://nl.wikipedia.org/wiki/Inwendig_product">inwendig product</a> tussen de matrix <span class="math inline">\(x\)</span> en de vector <span class="math inline">\(\theta\)</span>. Zie <a href="#inwendig-product-van-twee-vectoren">Appendix</a> voor meer uitleg hierover.</p>
</div>
<div id="het-leeralgoritme-van-de-perceptron" class="section level2">
<h2><span class="header-section-number">6.6</span> Het leeralgoritme van de perceptron</h2>
<p>De rest van de noodzakelijke code is a.h.w. van administratieve aard:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="de-percetron.html#cb60-1"></a><span class="kw">def</span> fit(<span class="va">self</span>, x: np.array, y: np.array):</span>
<span id="cb60-2"><a href="de-percetron.html#cb60-2"></a>    <span class="va">self</span>.theta <span class="op">=</span> np.zeros(x.shape[<span class="dv">1</span>])</span>
<span id="cb60-3"><a href="de-percetron.html#cb60-3"></a></span>
<span id="cb60-4"><a href="de-percetron.html#cb60-4"></a>    <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.iteration_count):</span>
<span id="cb60-5"><a href="de-percetron.html#cb60-5"></a>        err_iter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb60-6"><a href="de-percetron.html#cb60-6"></a>        <span class="cf">for</span> xi, yi <span class="kw">in</span> <span class="bu">zip</span>(x, y):</span>
<span id="cb60-7"><a href="de-percetron.html#cb60-7"></a>            update <span class="op">=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> (yi <span class="op">-</span> <span class="va">self</span>.t(<span class="va">self</span>.g(xi)))</span>
<span id="cb60-8"><a href="de-percetron.html#cb60-8"></a>            <span class="va">self</span>.theta_0 <span class="op">+=</span> update</span>
<span id="cb60-9"><a href="de-percetron.html#cb60-9"></a>            <span class="va">self</span>.theta <span class="op">+=</span> update <span class="op">*</span> xi</span>
<span id="cb60-10"><a href="de-percetron.html#cb60-10"></a>            err_iter <span class="op">+=</span> <span class="bu">int</span>(update <span class="op">!=</span> <span class="fl">0.0</span>)</span>
<span id="cb60-11"><a href="de-percetron.html#cb60-11"></a>        <span class="va">self</span>.errs.append(err_iter)</span></code></pre></div>
<p>Laten we de code even overlopen.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="de-percetron.html#cb61-1"></a><span class="va">self</span>.theta <span class="op">=</span> np.zeros(x.shape[<span class="dv">1</span>])</span></code></pre></div>
<p>Voor elke instantie in de invoer matrix <span class="math inline">\(x\)</span> wordt er een gewicht <span class="math inline">\(\theta_i\)</span> voorzien en op nul geïnitialiseerd. <code>shape</code> geeft de dimensies terug van een matrix en de eerste waarde geeft het aantal rijen.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="de-percetron.html#cb62-1"></a><span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.iteration_count):</span>
<span id="cb62-2"><a href="de-percetron.html#cb62-2"></a>    err_iter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb62-3"><a href="de-percetron.html#cb62-3"></a>    <span class="cf">for</span> xi, yi <span class="kw">in</span> <span class="bu">zip</span>(x, y):</span>
<span id="cb62-4"><a href="de-percetron.html#cb62-4"></a>        <span class="co"># Updaten van de gewichten en aantal fouten tellen</span></span>
<span id="cb62-5"><a href="de-percetron.html#cb62-5"></a>    <span class="va">self</span>.errs.append(err_iter)</span></code></pre></div>
<p>Tijdens elke cyclus wordt het aantal foute voorspellingen binnen de cyclus bijgehouden. De <code>zip</code> functie plakt telkens de werkelijke uitkomst aan een instantie. In ons geval zal <span class="math inline">\(x_1\)</span> bestaan uit de breedte van de kelkbladeren van de irissen en <span class="math inline">\(x_2\)</span> de lengte van de kelkbladeren bevatten. Dus, stel breedte = 3.5mm, lengte = 5.1mm en soort = <em>setosa</em>, dan verwachten we de eerste keer dat de compiler in de lus <code>python for xi, yi in zip(x, y):</code> terecht komt de volgende waarden:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="de-percetron.html#cb63-1"></a>xi <span class="op">=</span> [<span class="fl">3.5</span>, <span class="fl">5.1</span>]</span>
<span id="cb63-2"><a href="de-percetron.html#cb63-2"></a>yi <span class="op">=</span> <span class="dv">1</span></span></code></pre></div>
<p>Binnen in deze lus, wordt er berekend met hoeveel de parameters moeten worden verminderd:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="de-percetron.html#cb64-1"></a>update <span class="op">=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> (yi <span class="op">-</span> <span class="va">self</span>.t(<span class="va">self</span>.g(xi)))</span></code></pre></div>
<p>Stel dat het leeralgoritme voor een bepaalde instantie <code>1</code> voorspelt, maar dat de werkelijke uitkomst <code>-1</code> is, dan worden de parameters <em>vermeerderd</em> met de fractie</p>
<p>hetgeen overeenkomt met <span class="math inline">\(0.1\cdot\left(-1\cdot-1\right)=-0.2\)</span> voor een leersnelheid van <span class="math inline">\(0.1\)</span>. Met andere woorden, de parameters worden <em>verminderd</em> met één vijfde van hun waarde. Stel nu dat het leeralgoritme het juist had, dan wordt <code>update</code> gelijkgesteld aan <span class="math inline">\(0.1\cdot\left(1\cdot-1\right)=0\)</span> en blijven de parameters onveranderd.</p>
<p>En dit was het! Hier is de volledige werkende Python code:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="de-percetron.html#cb65-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb65-2"><a href="de-percetron.html#cb65-2"></a></span>
<span id="cb65-3"><a href="de-percetron.html#cb65-3"></a><span class="kw">class</span> Perceptron:</span>
<span id="cb65-4"><a href="de-percetron.html#cb65-4"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, learning_rate <span class="op">=</span> <span class="fl">0.1</span>, iteration_count <span class="op">=</span> <span class="dv">10</span>):</span>
<span id="cb65-5"><a href="de-percetron.html#cb65-5"></a>    <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb65-6"><a href="de-percetron.html#cb65-6"></a>    <span class="va">self</span>.iteration_count <span class="op">=</span> iteration_count</span>
<span id="cb65-7"><a href="de-percetron.html#cb65-7"></a>    <span class="va">self</span>.theta_0 <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb65-8"><a href="de-percetron.html#cb65-8"></a>    <span class="va">self</span>.theta <span class="op">=</span> <span class="va">None</span></span>
<span id="cb65-9"><a href="de-percetron.html#cb65-9"></a>    <span class="va">self</span>.errs <span class="op">=</span> []</span>
<span id="cb65-10"><a href="de-percetron.html#cb65-10"></a></span>
<span id="cb65-11"><a href="de-percetron.html#cb65-11"></a>  <span class="kw">def</span> g(<span class="va">self</span>, x: np.array) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb65-12"><a href="de-percetron.html#cb65-12"></a>    <span class="cf">return</span> np.dot(x, <span class="va">self</span>.theta) <span class="op">+</span> <span class="va">self</span>.theta_0</span>
<span id="cb65-13"><a href="de-percetron.html#cb65-13"></a></span>
<span id="cb65-14"><a href="de-percetron.html#cb65-14"></a>  <span class="kw">def</span> t(<span class="va">self</span>, z: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb65-15"><a href="de-percetron.html#cb65-15"></a>    <span class="cf">return</span> np.where(z <span class="op">&gt;=</span> <span class="fl">0.0</span>, <span class="dv">1</span>, <span class="dv">-1</span>)</span>
<span id="cb65-16"><a href="de-percetron.html#cb65-16"></a></span>
<span id="cb65-17"><a href="de-percetron.html#cb65-17"></a>  <span class="kw">def</span> fit(<span class="va">self</span>, x: np.array, y: np.array):</span>
<span id="cb65-18"><a href="de-percetron.html#cb65-18"></a>    <span class="va">self</span>.theta <span class="op">=</span> np.zeros(x.shape[<span class="dv">1</span>])</span>
<span id="cb65-19"><a href="de-percetron.html#cb65-19"></a></span>
<span id="cb65-20"><a href="de-percetron.html#cb65-20"></a>    <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.iteration_count):</span>
<span id="cb65-21"><a href="de-percetron.html#cb65-21"></a>      err_iter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb65-22"><a href="de-percetron.html#cb65-22"></a>      <span class="cf">for</span> xi, yi <span class="kw">in</span> <span class="bu">zip</span>(x, y):</span>
<span id="cb65-23"><a href="de-percetron.html#cb65-23"></a>        update <span class="op">=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> (yi <span class="op">-</span> <span class="va">self</span>.t(<span class="va">self</span>.g(xi)))</span>
<span id="cb65-24"><a href="de-percetron.html#cb65-24"></a>        <span class="va">self</span>.theta_0 <span class="op">+=</span> update</span>
<span id="cb65-25"><a href="de-percetron.html#cb65-25"></a>        <span class="va">self</span>.theta <span class="op">+=</span> update <span class="op">*</span> xi</span>
<span id="cb65-26"><a href="de-percetron.html#cb65-26"></a>        err_iter <span class="op">+=</span> <span class="bu">int</span>(update <span class="op">!=</span> <span class="fl">0.0</span>)</span>
<span id="cb65-27"><a href="de-percetron.html#cb65-27"></a>      <span class="va">self</span>.errs.append(err_iter)</span></code></pre></div>
</div>
<div id="trainen-van-de-perceptron" class="section level2">
<h2><span class="header-section-number">6.7</span> Trainen van de perceptron</h2>
<p>De tijd is aangebroken om de perceptron te trainen met de data van de iris bloemen. Het enige wat we nog willen doen is de invoergegevens standaardiseren, i.e. de zogenaamde <a href="https://nl.wikipedia.org/wiki/Z-score">Z-score</a> berekenen. Let wel deze Z <em>heeft niets te maken</em> met de <span class="math inline">\(z\)</span> (de <em>logit</em>) die we eerder zagen.</p>
<p><span class="math display" id="eq:z-score">\[\begin{equation}
  Z=\frac{x_j-\mu_j}{\sigma_j}
  \tag{6.4}
\end{equation}\]</span></p>
<p>Elke variabele <span class="math inline">\(x_j\)</span> wordt in dit process eerst verminderd met het gemiddelde voor die variabele <span class="math inline">\(\mu_j\)</span> en daarna gedeeld door de standaardafwijking van die variabele <span class="math inline">\(\sigma_j\)</span>. Wat de Z-score in feite doet is elke waarde transformeren naar het aantal standaardafwijkingen het verwijderd is van het gemiddelde.</p>
<p>De reden dat we de standaardisatie uitvoeren is omdat de lengte en de breedte van de kelkbladen een andere schaal hebben en daar kan dit eenvoudige leeralgoritme moeilijk mee overweg. Maar opgelet, het nemen van het gemiddelde en de standaardafwijking van een feature veronderstelt dat deze zich (toch ongeveer) normaal verdeeld. In de paragraaf over <a href="data-exploratie.html#univariate-verdelingen">Univariate verdelingen</a> staat beschreven hoe we een verdeling snel visueel kunnen controleren:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="de-percetron.html#cb66-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>)</span>
<span id="cb66-2"><a href="de-percetron.html#cb66-2"></a></span>
<span id="cb66-3"><a href="de-percetron.html#cb66-3"></a>iris<span class="op">$</span>Sepal.Width <span class="op">%&gt;%</span><span class="st"> </span>density <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">plot</span> (<span class="dt">main =</span> <span class="st">&quot;Kelkblad breedte&quot;</span>)</span>
<span id="cb66-4"><a href="de-percetron.html#cb66-4"></a>iris<span class="op">$</span>Sepal.Length <span class="op">%&gt;%</span><span class="st"> </span>density <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">plot</span> (<span class="dt">main =</span> <span class="st">&quot;Kelkblad lengte&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/iris-normaal-1.png" width="672" /></p>
<p>Toegegeven, de verdelingen wijken enigszins af van de normaalverdeling, maar het belangrijkste is dat beide variabelen ten minste unimodaal zijn, i.e. één ‘piek’ bezitten in hun verdeling. Dus een standaardisatie lijkt gelegitimeerd. De onderstaande code verzamelt de invoer en de uitkomsten en voert de standaardisatie door in R met de functie <code>base::scale</code>:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="de-percetron.html#cb67-1"></a>x_all &lt;-<span class="st"> </span>iris[, <span class="kw">c</span>(<span class="st">&quot;Sepal.Width&quot;</span>, <span class="st">&quot;Sepal.Length&quot;</span>)] <span class="op">%&gt;%</span></span>
<span id="cb67-2"><a href="de-percetron.html#cb67-2"></a><span class="st">  </span>as.matrix <span class="op">%&gt;%</span><span class="st"> </span>scale</span>
<span id="cb67-3"><a href="de-percetron.html#cb67-3"></a></span>
<span id="cb67-4"><a href="de-percetron.html#cb67-4"></a>y_all &lt;-<span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(iris<span class="op">$</span>Species <span class="op">==</span><span class="st"> &quot;setosa&quot;</span>)) <span class="op">-</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb67-5"><a href="de-percetron.html#cb67-5"></a></span>
<span id="cb67-6"><a href="de-percetron.html#cb67-6"></a>x_all <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">cbind</span>(y_all) <span class="op">%&gt;%</span><span class="st"> </span>head</span></code></pre></div>
<pre><code>##      Sepal.Width Sepal.Length y_all
## [1,]  1.01560199   -0.8976739     1
## [2,] -0.13153881   -1.1392005     1
## [3,]  0.32731751   -1.3807271     1
## [4,]  0.09788935   -1.5014904     1
## [5,]  1.24503015   -1.0184372     1
## [6,]  1.93331463   -0.5353840     1</code></pre>
<p>Er rest nu alleen nog het algoritme te trainen. Hiervoor gaan we de invoer data randomiseren in een training- en een test-set in de verhouding <span class="math inline">\(3:1\)</span>.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="de-percetron.html#cb69-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb69-2"><a href="de-percetron.html#cb69-2"></a></span>
<span id="cb69-3"><a href="de-percetron.html#cb69-3"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb69-4"><a href="de-percetron.html#cb69-4"></a>  r.x_all, r.y_all, test_size <span class="op">=</span> <span class="fl">0.25</span>, random_state <span class="op">=</span> <span class="dv">42</span>)</span></code></pre></div>
<p>De eigenlijke training gebeurt als volgt:</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="de-percetron.html#cb70-1"></a>model <span class="op">=</span> Perceptron(learning_rate <span class="op">=</span> <span class="fl">0.001</span>)</span>
<span id="cb70-2"><a href="de-percetron.html#cb70-2"></a>model.fit(x_train, y_train)</span></code></pre></div>
<p>Laten we eens kijken hoe het aantal mis-classificaties evolueerde over de verschillende cycli (deze cycli worden in ANN-wereld <em>epochs</em> genoemd):</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="de-percetron.html#cb71-1"></a>py<span class="op">$</span>model<span class="op">$</span>errs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">plot</span>(<span class="dt">type =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">cex =</span> <span class="fl">.8</span>,</span>
<span id="cb71-2"><a href="de-percetron.html#cb71-2"></a>  <span class="dt">ylab =</span> <span class="st">&quot;# fouten per epoch&quot;</span>, <span class="dt">xlab  =</span> <span class="st">&quot;Epoch&quot;</span>,</span>
<span id="cb71-3"><a href="de-percetron.html#cb71-3"></a>  <span class="dt">main =</span> <span class="st">&quot;Evolutie misclassificaties&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/misclassificaties-1.png" width="672" /></p>
<p>Nu kunnen we de scheidingslijn in beeld brengen. Eerst maken we een raster. Voor elke punt in dit raster zullen we een voorspelling maken:</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="de-percetron.html#cb72-1"></a>x1_lim &lt;-<span class="st"> </span>x_all[, <span class="dv">1</span>] <span class="op">%&gt;%</span><span class="st"> </span>range</span>
<span id="cb72-2"><a href="de-percetron.html#cb72-2"></a>x2_lim &lt;-<span class="st"> </span>x_all[, <span class="dv">2</span>] <span class="op">%&gt;%</span><span class="st"> </span>range</span>
<span id="cb72-3"><a href="de-percetron.html#cb72-3"></a></span>
<span id="cb72-4"><a href="de-percetron.html#cb72-4"></a>grid_x &lt;-<span class="st"> </span><span class="kw">seq</span>(x1_lim[<span class="dv">1</span>], x1_lim[<span class="dv">2</span>], <span class="dt">l =</span> <span class="dv">100</span>)</span>
<span id="cb72-5"><a href="de-percetron.html#cb72-5"></a>grid_y &lt;-<span class="st"> </span><span class="kw">seq</span>(x2_lim[<span class="dv">1</span>], x2_lim[<span class="dv">2</span>], <span class="dt">l =</span> <span class="dv">100</span>)</span>
<span id="cb72-6"><a href="de-percetron.html#cb72-6"></a></span>
<span id="cb72-7"><a href="de-percetron.html#cb72-7"></a>grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">x =</span> grid_x, <span class="dt">y =</span> grid_y) <span class="op">%&gt;%</span><span class="st"> </span>as.matrix</span>
<span id="cb72-8"><a href="de-percetron.html#cb72-8"></a></span>
<span id="cb72-9"><a href="de-percetron.html#cb72-9"></a>grid_z &lt;-<span class="st"> </span>py<span class="op">$</span>model<span class="op">$</span><span class="kw">t</span>(py<span class="op">$</span>model<span class="op">$</span><span class="kw">g</span>(grid))</span></code></pre></div>
<p>Nu plotten we de (gestandaardiseerd) data met de scheidingslijn (eng: <em>boundary line</em>) en duiden we de gebieden aan die, volgens het model, tot <em>Iris setosa</em> behoort of niet:</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="de-percetron.html#cb73-1"></a><span class="kw">plot</span>(x_all[, <span class="dv">2</span>] <span class="op">~</span><span class="st"> </span>x_all[, <span class="dv">1</span>], <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">cex =</span> <span class="fl">.8</span>,</span>
<span id="cb73-2"><a href="de-percetron.html#cb73-2"></a>  <span class="dt">col =</span> <span class="dv">1</span> <span class="op">+</span><span class="st"> </span>(iris<span class="op">$</span>Species <span class="op">==</span><span class="st"> &quot;setosa&quot;</span>),</span>
<span id="cb73-3"><a href="de-percetron.html#cb73-3"></a>  <span class="dt">xlab =</span> <span class="st">&quot;Kelkblad breedte (gestand.)&quot;</span>,</span>
<span id="cb73-4"><a href="de-percetron.html#cb73-4"></a>  <span class="dt">ylab =</span> <span class="st">&quot;Kelkblad lengte (gestand.)&quot;</span>,</span>
<span id="cb73-5"><a href="de-percetron.html#cb73-5"></a>  <span class="dt">main =</span> <span class="st">&quot;Setosa vs {versicolor, virginica}&quot;</span>)</span>
<span id="cb73-6"><a href="de-percetron.html#cb73-6"></a></span>
<span id="cb73-7"><a href="de-percetron.html#cb73-7"></a><span class="kw">curve</span>((py<span class="op">$</span>model<span class="op">$</span>theta[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>py<span class="op">$</span>model<span class="op">$</span>theta_<span class="dv">0</span>) <span class="op">/</span><span class="st"> </span><span class="op">-</span>py<span class="op">$</span>model<span class="op">$</span>theta[<span class="dv">2</span>], <span class="dt">add =</span> <span class="ot">TRUE</span>)</span>
<span id="cb73-8"><a href="de-percetron.html#cb73-8"></a></span>
<span id="cb73-9"><a href="de-percetron.html#cb73-9"></a><span class="kw">points</span>(grid[, <span class="dv">1</span>], grid[, <span class="dv">2</span>], <span class="dt">pch =</span> <span class="st">&quot;.&quot;</span>, <span class="dt">col =</span> <span class="dv">1</span> <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>grid_z) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)</span>
<span id="cb73-10"><a href="de-percetron.html#cb73-10"></a></span>
<span id="cb73-11"><a href="de-percetron.html#cb73-11"></a><span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;{versicolor, virginica}&quot;</span>, <span class="st">&quot;setosa&quot;</span>),</span>
<span id="cb73-12"><a href="de-percetron.html#cb73-12"></a>  <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="dt">pch =</span> <span class="dv">19</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/iris-preds-1.png" width="672" /></p>
<p>Merk nog eens op dat het resultaat van ML inderdaad een functie is. In dit geval moet je de twee functies <span class="math inline">\(f\)</span> en <span class="math inline">\(t\)</span> achtereenvolgens uitvoeren. In de praktijk worden beide meestal verpakt in een functie <code>predict</code>, maar hier is ervoor gekozen om de functies gescheiden te houden, kwestie om de relatie te behouden met de Formules <a href="de-percetron.html#eq:perceptron">(6.1)</a> en <a href="de-percetron.html#eq:perceptron-transformatie">(6.2)</a>.</p>
</div>
<div id="voorspellen-van-de-iris-soort" class="section level2">
<h2><span class="header-section-number">6.8</span> Voorspellen van de iris soort</h2>
<p>Laten we nu het model gebruiken om een voorspelling te maken op ongeziene data.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="de-percetron.html#cb74-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb74-2"><a href="de-percetron.html#cb74-2"></a></span>
<span id="cb74-3"><a href="de-percetron.html#cb74-3"></a><span class="bu">print</span>(accuracy_score(model.t(model.g(x_test)), y_test))</span></code></pre></div>
<pre><code>## 1.0</code></pre>
<p>Een perfect resultaat!</p>

</div>
</div>
<h3>Bronvermelding</h3>
<div id="refs" class="references">
<div id="ref-neuron">
<p>Commons, W., 2013. Multipolar neuron [WWW Document] <em>[Online; accessed 2020-09-07]</em>. URL <a href="https://upload.wikimedia.org/wikipedia/commons/1/10/Blausen_0657_MultipolarNeuron.png">https://upload.wikimedia.org/wikipedia/commons/1/10/Blausen_0657_MultipolarNeuron.png</a></p>
</div>
<div id="ref-mark-1">
<p>Cornell University Library, n.d. IBM 704 mainframe [WWW Document] <em>[Online; accessed 2020-09-29]</em>. URL <a href="https://upload.wikimedia.org/wikipedia/en/5/52/Mark_I_perceptron.jpeg">https://upload.wikimedia.org/wikipedia/en/5/52/Mark_I_perceptron.jpeg</a></p>
</div>
<div id="ref-ibm-704">
<p>Lawrence Livermore National Laboratory, n.d. IBM 704 mainframe [WWW Document] <em>[Online; accessed 2020-09-29]</em>. URL <a href="https://commons.wikimedia.org/wiki/File:IBM_704_mainframe.gif">https://commons.wikimedia.org/wiki/File:IBM_704_mainframe.gif</a></p>
</div>
<div id="ref-iris-virginica">
<p>Mayfield, F., 2007. Image of iris virginica shrevei at the james woodworth prairie preserve - a bud and a single flower at full bloom [WWW Document] <em>[Online; accessed 2020-09-07]</em>. URL <a href="https://commons.wikimedia.org/wiki/File:Iris_virginica.jpg">https://commons.wikimedia.org/wiki/File:Iris_virginica.jpg</a></p>
</div>
<div id="ref-iris-versicolor">
<p>Mayfield, F., 2005. Blue flag flower close-up (iris versicolor) at the forillon national park of canada [WWW Document] <em>[Online; accessed 2020-09-07]</em>. URL <a href="https://commons.wikimedia.org/wiki/File:Iris_versicolor_3.jpg">https://commons.wikimedia.org/wiki/File:Iris_versicolor_3.jpg</a></p>
</div>
<div id="ref-perceptron-scratch">
<p>Peirone, S.A., 2007. Image of iris setosa in botanical garden in poznań [WWW Document] <em>[Online; accessed 2020-09-07]</em>. URL <a href="https://commons.wikimedia.org/wiki/File:Kosaciec_szczecinkowaty_Iris_setosa.jpg">https://commons.wikimedia.org/wiki/File:Kosaciec_szczecinkowaty_Iris_setosa.jpg</a></p>
</div>
<div id="ref-iris-setosa">
<p>Szczecinkowaty, K., 2007. Image of iris setosa in botanical garden in poznań [WWW Document] <em>[Online; accessed 2020-09-07]</em>. URL <a href="https://commons.wikimedia.org/wiki/File:Kosaciec_szczecinkowaty_Iris_setosa.jpg">https://commons.wikimedia.org/wiki/File:Kosaciec_szczecinkowaty_Iris_setosa.jpg</a></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="manipuleren-van-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inleiding-tot-artificiële-neurale-netwerken.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/ddhaese/machine-learning-source/06_Perceptron.Rmd",
"text": "Bron"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
